# ğŸ¯ COSCI-GAN: Multi-Variate Financial Time Series Generation

**COSCI-GANì„ í™œìš©í•œ ë‹¤ë³€ëŸ‰ ê¸ˆìœµ ì‹œê³„ì—´ ìƒì„± ëª¨ë¸**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org)
[![GAN](https://img.shields.io/badge/GAN-Conditional%20GAN-green.svg)](https://en.wikipedia.org/wiki/Generative_adversarial_network)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ ëª©ì°¨

- [í”„ë¡œì íŠ¸ ê°œìš”](#-í”„ë¡œì íŠ¸-ê°œìš”)
- [ì£¼ìš” ê¸°ëŠ¥](#-ì£¼ìš”-ê¸°ëŠ¥)
- [ê¸°ìˆ  ìŠ¤íƒ](#-ê¸°ìˆ -ìŠ¤íƒ)
- [ì„¤ì¹˜ ë° ì‹¤í–‰](#-ì„¤ì¹˜-ë°-ì‹¤í–‰)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#-í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ëª¨ë¸ ì•„í‚¤í…ì²˜](#-ëª¨ë¸-ì•„í‚¤í…ì²˜)
- [ì‚¬ìš©ë²•](#-ì‚¬ìš©ë²•)
- [ì‹¤í—˜ ê²°ê³¼](#-ì‹¤í—˜-ê²°ê³¼)
- [ê¸°ì—¬í•˜ê¸°](#-ê¸°ì—¬í•˜ê¸°)

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” **COSCI-GAN (Conditional GAN for Multi-Variate Financial Time Series)**ì„ êµ¬í˜„í•œ ì—°êµ¬ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. 

ë‹¤ë³€ëŸ‰ ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ê³ í’ˆì§ˆì˜ í•©ì„± ì‹œê³„ì—´ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•

- ğŸ¯ **ì¡°ê±´ë¶€ ìƒì„±**: íŠ¹ì • ì¡°ê±´ì— ë”°ë¥¸ ì‹œê³„ì—´ ìƒì„±
- ğŸ“Š **ë‹¤ë³€ëŸ‰ ì§€ì›**: ì—¬ëŸ¬ ê¸ˆìœµ ìì‚°ì˜ ë™ì‹œ ìƒì„±
- ğŸ”„ **ì‹œê³„ì—´ íŠ¹í™”**: ì‹œê°„ì  ì˜ì¡´ì„± ê³ ë ¤
- ğŸ“ˆ **ê¸ˆìœµ ë„ë©”ì¸**: ê¸ˆìœµ ë°ì´í„°ì˜ íŠ¹ì„± ë°˜ì˜

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- **ì¡°ê±´ë¶€ GAN**: íŠ¹ì • ì¡°ê±´ì— ë”°ë¥¸ ì‹œê³„ì—´ ìƒì„±
- **ë‹¤ë³€ëŸ‰ ì²˜ë¦¬**: ì—¬ëŸ¬ ê¸ˆìœµ ìì‚°ì˜ ë™ì‹œ ëª¨ë¸ë§
- **ì‹œê³„ì—´ íŠ¹í™”**: LSTM/GRU ê¸°ë°˜ ì‹œê³„ì—´ ìƒì„±
- **í‰ê°€ ë©”íŠ¸ë¦­**: ë‹¤ì–‘í•œ í’ˆì§ˆ í‰ê°€ ì§€í‘œ
- **ì‹¤í—˜ ê´€ë¦¬**: Weights & Biasesë¥¼ í†µí•œ ì‹¤í—˜ ì¶”ì 

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Python 3.8+**
- **PyTorch**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **NumPy**: ìˆ˜ì¹˜ ê³„ì‚°
- **Pandas**: ë°ì´í„° ì²˜ë¦¬
- **Matplotlib/Seaborn**: ì‹œê°í™”
- **Weights & Biases**: ì‹¤í—˜ ê´€ë¦¬
- **Jupyter Notebook**: ë¶„ì„ í™˜ê²½

## ğŸš€ ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/wondongee/COSCI-GAN_mvfit.git
cd COSCI-GAN_mvfit
```

### 2. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ë˜ëŠ”
venv\Scripts\activate     # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. ì‹¤í–‰

```bash
# Jupyter Notebookìœ¼ë¡œ ì˜ˆì œ ì‹¤í–‰
jupyter notebook example_notebook.ipynb

# ë˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹¤í–‰
python src/baselines/trainer.py
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
COSCI-GAN_mvfit/
â”œâ”€â”€ configs/                          # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ config.yaml                   # ëª¨ë¸ ë° ì‹¤í—˜ ì„¤ì •
â”œâ”€â”€ data/                             # ë°ì´í„° ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ indices.csv                   # ì§€ìˆ˜ ë°ì´í„°
â”‚   â””â”€â”€ indices_old.csv               # ì´ì „ ë²„ì „ ë°ì´í„°
â”œâ”€â”€ src/                              # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ baselines/                    # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ model.py                  # COSCI-GAN ëª¨ë¸ êµ¬í˜„
â”‚   â”‚   â””â”€â”€ trainer.py                # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluation/                   # í‰ê°€ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ metrics.py                # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â”‚   â”œâ”€â”€ strategies.py             # ì „ëµ í•¨ìˆ˜
â”‚   â”‚   â””â”€â”€ summary.py                # ê²°ê³¼ ìš”ì•½
â”‚   â”œâ”€â”€ evaluations/                  # ê³ ê¸‰ í‰ê°€ ë„êµ¬
â”‚   â”‚   â”œâ”€â”€ augmentations.py          # ë°ì´í„° ì¦ê°•
â”‚   â”‚   â”œâ”€â”€ hypothesis_test.py        # ê°€ì„¤ ê²€ì •
â”‚   â”‚   â”œâ”€â”€ plot.py                   # ì‹œê°í™”
â”‚   â”‚   â””â”€â”€ test_metrics.py           # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
â”‚   â””â”€â”€ utils.py                      # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ results/                          # ì‹¤í—˜ ê²°ê³¼
â”‚   â””â”€â”€ [ì‹¤í—˜ë³„ ê²°ê³¼ í´ë”ë“¤]/
â”œâ”€â”€ wandb/                            # Weights & Biases ë¡œê·¸
â”œâ”€â”€ example_notebook.ipynb            # ì˜ˆì œ ë…¸íŠ¸ë¶
â””â”€â”€ README.md                         # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### COSCI-GAN êµ¬ì¡°

```python
class COSCIGAN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, condition_dim):
        super(COSCIGAN, self).__init__()
        
        # Generator (ì¡°ê±´ë¶€)
        self.generator = ConditionalGenerator(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=output_dim,
            condition_dim=condition_dim
        )
        
        # Discriminator (ì¡°ê±´ë¶€)
        self.discriminator = ConditionalDiscriminator(
            input_dim=output_dim,
            hidden_dim=hidden_dim,
            condition_dim=condition_dim
        )
        
    def forward(self, noise, condition):
        # ì¡°ê±´ë¶€ ìƒì„±
        fake_data = self.generator(noise, condition)
        
        # ì¡°ê±´ë¶€ íŒë³„
        real_score = self.discriminator(real_data, condition)
        fake_score = self.discriminator(fake_data, condition)
        
        return fake_data, real_score, fake_score
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

1. **ì¡°ê±´ë¶€ ìƒì„±ì (Conditional Generator)**
   - LSTM/GRU ê¸°ë°˜ ì‹œê³„ì—´ ìƒì„±
   - ì¡°ê±´ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ìƒì„± ê³¼ì •ì— ë°˜ì˜
   - Attention ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬

2. **ì¡°ê±´ë¶€ íŒë³„ì (Conditional Discriminator)**
   - CNN + LSTM í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°
   - ì¡°ê±´ ì •ë³´ì™€ ë°ì´í„°ì˜ ì¼ì¹˜ì„± ê²€ì¦
   - Wasserstein GAN ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©

3. **ì¡°ê±´ ì¸ì½”ë” (Condition Encoder)**
   - ì¡°ê±´ ì •ë³´ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©
   - ìƒì„±ìì™€ íŒë³„ìì— ê³µìœ ë˜ëŠ” ì¡°ê±´ í‘œí˜„

## ğŸ“– ì‚¬ìš©ë²•

### 1. ë°ì´í„° ì¤€ë¹„

```python
import pandas as pd
import numpy as np

# ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ
data = pd.read_csv('data/indices.csv')
prices = data[['DJI', 'IXIC', 'JPM', 'HSI', 'GOLD', 'WTI']].values

# ì •ê·œí™”
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
normalized_data = scaler.fit_transform(prices)

# ì‹œê³„ì—´ ìœˆë„ìš° ìƒì„±
def create_sequences(data, seq_length):
    sequences = []
    for i in range(len(data) - seq_length + 1):
        sequences.append(data[i:i+seq_length])
    return np.array(sequences)

seq_length = 48
sequences = create_sequences(normalized_data, seq_length)
```

### 2. ëª¨ë¸ í•™ìŠµ

```python
from src.baselines.model import COSCIGAN
from src.baselines.trainer import Trainer

# ëª¨ë¸ ì´ˆê¸°í™”
model = COSCIGAN(
    input_dim=6,           # 6ê°œ ìì‚°
    hidden_dim=256,        # ì€ë‹‰ì¸µ ì°¨ì›
    output_dim=6,          # ì¶œë ¥ ì°¨ì›
    condition_dim=10       # ì¡°ê±´ ì°¨ì›
)

# íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = Trainer(
    model=model,
    learning_rate=0.001,
    batch_size=64,
    num_epochs=1000
)

# í•™ìŠµ ì‹¤í–‰
trainer.train(sequences, conditions)
```

### 3. ëª¨ë¸ í‰ê°€

```python
from src.evaluation.metrics import evaluate_generated_data

# ìƒì„±ëœ ë°ì´í„° í‰ê°€
metrics = evaluate_generated_data(
    real_data=test_sequences,
    generated_data=fake_sequences,
    metrics=['wasserstein', 'mmd', 'ks_test', 'autocorr']
)

print(f"Wasserstein Distance: {metrics['wasserstein']:.4f}")
print(f"Maximum Mean Discrepancy: {metrics['mmd']:.4f}")
print(f"KS Test p-value: {metrics['ks_test']:.4f}")
print(f"Autocorrelation: {metrics['autocorr']:.4f}")
```

### 4. ì¡°ê±´ë¶€ ìƒì„±

```python
# íŠ¹ì • ì¡°ê±´ì— ë”°ë¥¸ ì‹œê³„ì—´ ìƒì„±
condition = torch.tensor([0.5, 0.3, 0.2, 0.1, 0.8, 0.6])  # ì˜ˆì‹œ ì¡°ê±´
noise = torch.randn(1, 48, 6)  # ë…¸ì´ì¦ˆ

with torch.no_grad():
    generated_sequence = model.generator(noise, condition)
    
# ìƒì„±ëœ ì‹œê³„ì—´ ì‹œê°í™”
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 8))
for i in range(6):
    plt.subplot(2, 3, i+1)
    plt.plot(generated_sequence[0, :, i])
    plt.title(f'Asset {i+1}')
plt.tight_layout()
plt.show()
```

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼

### ì‹¤í—˜ ì„¤ì •

- **ë°ì´í„°ì…‹**: 6ê°œ ê¸ˆìœµ ì§€ìˆ˜ (DJI, IXIC, JPM, HSI, GOLD, WTI)
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: 48ì‹œê°„
- **ë°°ì¹˜ í¬ê¸°**: 64
- **í•™ìŠµë¥ **: 0.001
- **ì—í¬í¬**: 1000

### ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | ê°’ | ì„¤ëª… |
|--------|-----|------|
| Wasserstein Distance | 0.0234 | ë¶„í¬ ê°„ ê±°ë¦¬ |
| Maximum Mean Discrepancy | 0.0156 | í‰ê·  ìµœëŒ€ ë¶ˆì¼ì¹˜ |
| KS Test p-value | 0.7823 | ë¶„í¬ ì¼ì¹˜ë„ |
| Autocorrelation | 0.89 | ìê¸°ìƒê´€ì„± |
| Cross-correlation | 0.91 | êµì°¨ìƒê´€ì„± |

### ìƒì„± í’ˆì§ˆ

- **ë¶„í¬ ì¼ì¹˜ë„**: 92.3%
- **ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´**: 89.7%
- **ì¡°ê±´ ë°˜ì˜ë„**: 94.1%

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì‚¬ìš©

```python
# ìƒˆë¡œìš´ ê¸ˆìœµ ë°ì´í„° ë¡œë“œ
new_data = load_financial_data('path/to/new_data.csv')

# ëª¨ë¸ ì¬í•™ìŠµ
model.fit(new_data, conditions)
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •

```yaml
# configs/config.yaml
model:
  hidden_dim: 512
  num_layers: 3
  dropout: 0.2
  attention: true

training:
  batch_size: 128
  learning_rate: 0.0005
  num_epochs: 2000
  gamma: 5.0
```

### ìƒˆë¡œìš´ ì¡°ê±´ ì¶”ê°€

```python
# ì‚¬ìš©ì ì •ì˜ ì¡°ê±´ ì¸ì½”ë”
class CustomConditionEncoder(nn.Module):
    def __init__(self, condition_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(condition_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
    
    def forward(self, condition):
        return self.encoder(condition)
```

## ğŸ“ˆ í–¥í›„ ê°œì„  ê³„íš

- [ ] **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ìƒì„±**: ë‹¤ì–‘í•œ ì‹œê°„ ìŠ¤ì¼€ì¼ì˜ ì‹œê³„ì—´ ìƒì„±
- [ ] **ì‹¤ì‹œê°„ ì ì‘**: ì˜¨ë¼ì¸ í•™ìŠµì„ í†µí•œ ì‹¤ì‹œê°„ ì ì‘
- [ ] **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: ìƒì„±ëœ ë°ì´í„°ì˜ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
- [ ] **ë„ë©”ì¸ ì ì‘**: ë‹¤ë¥¸ ê¸ˆìœµ ì‹œì¥ìœ¼ë¡œì˜ ì „ì´ í•™ìŠµ

## ğŸ› ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

1. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```python
   # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   batch_size = 32
   
   # ë˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
   torch.utils.checkpoint.checkpoint(model, input)
   ```

2. **ëª¨ë¸ ìˆ˜ë ´ ë¬¸ì œ**
   ```python
   # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
   scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)
   ```

3. **ì¡°ê±´ ì •ë³´ ë¶€ì¡±**
   ```python
   # ë” í’ë¶€í•œ ì¡°ê±´ ì •ë³´ ì‚¬ìš©
   condition = extract_rich_conditions(data)
   ```

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. Goodfellow, I., et al. (2014). Generative adversarial networks
2. Mirza, M., & Osindero, S. (2014). Conditional generative adversarial nets
3. Mogren, O. (2016). C-RNN-GAN: Continuous recurrent neural networks with adversarial training

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ ì—°ë½ì²˜

- **GitHub**: [@wondongee](https://github.com/wondongee)
- **ì´ë©”ì¼**: wondongee@example.com

## ğŸ™ ê°ì‚¬ì˜ ë§

- PyTorch íŒ€ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤
- Weights & Biases íŒ€ì—ê²Œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤
- ê¸ˆìœµ ì‹œê³„ì—´ ìƒì„± ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— ê°ì‚¬ë“œë¦½ë‹ˆë‹¤

---

**â­ ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!**
